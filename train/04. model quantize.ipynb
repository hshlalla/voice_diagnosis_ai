{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 가중치만 저장해서 모델 용량 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 18:19:16.606086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.637786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.637917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.638438: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 18:19:16.638947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.639055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.639132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.685728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.685856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.685943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 18:19:16.686041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:05:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_10_67.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_05_63.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_03_64.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_02_64.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_04_67.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_08_69.5.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_07_64.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_06_62.2.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_09_69.5.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_11_67.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/MA/save_weight_MvsA_01_68.3.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_06_72.0.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_01_73.2.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_04_75.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_09_65.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_03_82.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_10_70.7.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_11_74.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_07_75.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_05_74.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_08_80.5.h5\n",
      "Saved weights: /104data/models/spick_used_models/NA/save_weight_SvsA_02_68.3.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_07_74.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_01_70.7.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_02_66.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_03_70.7.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_09_67.2.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_05_73.3.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_06_75.0.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_08_75.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_11_69.0.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_10_68.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/NAB/save_weight_SvsO_04_68.1.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_10_65.6.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_11_66.7.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_05_68.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_02_68.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_03_67.8.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_04_64.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_07_67.8.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_09_64.4.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_01_70.0.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_06_68.9.h5\n",
      "Saved weights: /104data/models/spick_used_models/NM/save_weight_SvsM_08_75.6.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "bath_path = \"/104data/models/spick_used_models/\" #완성된 모델 경로\n",
    "\n",
    "for model_folder_name in [\"MA\", \"NA\", \"NAB\", \"NM\"]:\n",
    "    folder_path = os.path.join(bath_path, model_folder_name)\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.h5')]\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(folder_path, model_file)\n",
    "        # Load the model\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Modify the name to save weights only\n",
    "        weight_file_name = model_file.replace('model', 'weight')\n",
    "        weight_file_path = os.path.join(folder_path, weight_file_name)\n",
    "        \n",
    "        # Save weights\n",
    "        model.save_weights(weight_file_path)\n",
    "\n",
    "        print(f\"Saved weights: {weight_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 양자화를 통한 모델 줄이기\n",
    "tensorflow는 양자화를 진행하게되면 predict 하는 방식에 변화가 많이 생김. 양자화 후에는 모델 재훈련 불가 원본 모델은 형상관리를 위해 남겨둘것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpk225gxaj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpk225gxaj/assets\n",
      "2024-12-06 14:57:38.926398: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2024-12-06 14:57:38.926420: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2024-12-06 14:57:38.926569: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpk225gxaj\n",
      "2024-12-06 14:57:38.929061: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2024-12-06 14:57:38.929077: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpk225gxaj\n",
      "2024-12-06 14:57:38.938404: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2024-12-06 14:57:39.049533: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpk225gxaj\n",
      "2024-12-06 14:57:39.071007: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 144437 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model: /104data/models/spick_used_models/MA/save_model_MvsA_10_67.1_quantized.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_0953hn3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_0953hn3/assets\n",
      "2024-12-06 14:58:32.098250: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2024-12-06 14:58:32.098276: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2024-12-06 14:58:32.098435: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp_0953hn3\n",
      "2024-12-06 14:58:32.100869: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2024-12-06 14:58:32.100885: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmp_0953hn3\n",
      "2024-12-06 14:58:32.110507: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2024-12-06 14:58:32.207478: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmp_0953hn3\n",
      "2024-12-06 14:58:32.229305: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 130870 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model: /104data/models/spick_used_models/MA/save_model_MvsA_05_63.4_quantized.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmputistkw3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmputistkw3/assets\n",
      "2024-12-06 14:59:14.675597: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2024-12-06 14:59:14.675620: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2024-12-06 14:59:14.675761: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmputistkw3\n",
      "2024-12-06 14:59:14.678099: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2024-12-06 14:59:14.678113: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmputistkw3\n",
      "2024-12-06 14:59:14.687573: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2024-12-06 14:59:14.789692: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmputistkw3\n",
      "2024-12-06 14:59:14.810999: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 135239 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model: /104data/models/spick_used_models/MA/save_model_MvsA_03_64.6_quantized.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp8tuh3k2l/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8tuh3k2l/assets\n",
      "2024-12-06 14:59:56.510483: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2024-12-06 14:59:56.510509: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2024-12-06 14:59:56.510666: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp8tuh3k2l\n",
      "2024-12-06 14:59:56.512873: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2024-12-06 14:59:56.512889: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmp8tuh3k2l\n",
      "2024-12-06 14:59:56.521720: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2024-12-06 14:59:56.637345: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmp8tuh3k2l\n",
      "2024-12-06 14:59:56.657701: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 147035 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model: /104data/models/spick_used_models/MA/save_model_MvsA_02_64.6_quantized.tflite\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# 완성된 모델 경로\n",
    "bath_path = \"/104data/models/spick_used_models/\" \n",
    "\n",
    "# 양자화 및 변환을 진행할 폴더들\n",
    "for model_folder_name in [\"MA\", \"NA\", \"NAB\", \"NM\"]:\n",
    "    folder_path = os.path.join(bath_path, model_folder_name)\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.h5') and \"weight\" not in f]\n",
    "\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(folder_path, model_file)\n",
    "\n",
    "        # Load the model\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # TFLite 변환기 생성\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "        # 기본 양자화 (float32 -> float16 가중치로 줄임)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "        # 양자화 모델 변환\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # 양자화된 파일 저장 경로 생성\n",
    "        tflite_file_name = model_file.replace('.h5', '_quantized.tflite')\n",
    "        tflite_file_path = os.path.join(folder_path, tflite_file_name)\n",
    "\n",
    "        # 양자화 모델 저장\n",
    "        with open(tflite_file_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"Saved quantized model: {tflite_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
